# -*- coding: utf-8 -*-
"""SA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D1BsOAEWaMpSKOv5CifB0JoZ9hmtZW4U

# 1. Install Dependencies
"""

!pip install clean-text unidecode hazm huggingface_hub

"""# 2. Imports"""

import torch
from torch.utils.data import Dataset
import os
from sklearn.metrics import precision_recall_fscore_support
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import get_linear_schedule_with_warmup
import re
import string

import nltk
from cleantext.clean import remove_emoji as clean_text_remove_emoji
from hazm import Normalizer as HazmNormalizer
from hazm import stopwords_list
from nltk.corpus import stopwords

import pandas as pd
from sklearn.model_selection import train_test_split
from huggingface_hub import notebook_login

"""# Utils

## Constants
"""

base_path = '/content'
if not os.path.exists(f'{base_path}/stats'):
  os.mkdir(f'{base_path}/stats')

# Map classes to integers
label_dict = {
    "HAPPY": 0,  # in article: happiness (با مقاله فرق داره)
    "SAD": 1,
    "ANGRY": 2,
    "FEAR": 3,
    "SURPRISE": 4,
    "HATE": 5,
    "OTHER": 6
}

"""## Preprocess"""

# first we tried dadmatech tools but does not work well, so we implement our own preprocess
# we wanted to implement but did not have time (future works)
def replace_emojis(text):
    # Happy
    grin = 'خنده'
    laugh = 'خنده'
    happy = 'خوشحال'
    _text = re.sub(":D", grin, text)
    _text = re.sub(" (x|X)D", laugh, _text)
    _text = re.sub(":\)+", happy, _text)

    # Sad
    sad = 'ناراحت'
    annoyed = 'رنجیده'
    _text = re.sub(":\(+", sad, _text)
    _text = re.sub("-_+-", annoyed, _text)
    return _text


def remove_emojis(text):
    _text = clean_text_remove_emoji(text)
    return _text


def remove_url(text):
    _text = re.sub(r"https?:\S+", '', text)
    return _text


def remove_punc(text):
    _text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuations from text using string.punctuation
    persian_virgol = '،'  # noqa
    _text = _text.replace(persian_virgol, ' ')
    return _text


def remove_numbers(text):
    _text = re.sub(r'\d+', '', text)
    return _text


def remove_hashtags(text):
    _text = re.sub(r'#\S+', '', text)
    return _text


def remove_mentions(text):
    _text = re.sub(r'@\S+', '', text)
    return _text


def remove_duplicate_spaces(text):
    _text = " ".join(text.split())
    return _text


def clean_text(text) -> str:
    _text = remove_punc(
        remove_numbers(
            remove_mentions(
                remove_hashtags(
                    remove_duplicate_spaces(
                        remove_url(
                            remove_emojis(text)
                        )
                    )
                )
            )
        )
    )

    normalizer = HazmNormalizer() # we use hazm for normalizing the text (removing extra spaces, etc.) (e.g. "می‌روم" -> "می روم", "خوبییییییییییییییی؟" -> "خوبی", "خوبی" -> "خوبی؟")
    _text = normalizer.normalize(_text)

    return _text


# def combined_preprocess(text: str) -> str:
#     normalizer = Normalizer(full_cleaning=True)
#     normalizer.remove_stop_word = False  # if it's True, it reduces the accuracy
#     normalizer.remove_puncs = False  # we remove punctuations in clean_text function
#     normalized_text = normalizer.normalize(text)
#     return clean_text(normalized_text)

"""## Dataset"""

class SentenceDataset(Dataset):  # Create a custom dataset suitable for the task with bert
    def __init__(self, sentences, labels, tokenizer, label_dict):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.label2idx = {label: idx for label, idx in label_dict.items()}

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = str(self.sentences[idx])
        label = self.label2idx[self.labels[idx]]

        encoding = self.tokenizer.encode_plus(
            sentence,
            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'
            # max_length=512,  # Pad or truncate all sentences to the same length
            max_length=128,
            padding="max_length",  # Add padding to the sentences
            truncation=True,  # Truncate sentences that exceed the max length
            return_tensors="pt",  # Return PyTorch tensors
        )

        input_ids = encoding["input_ids"].squeeze()  # Remove the batch dimension
        attention_mask = encoding["attention_mask"].squeeze()  # Remove the batch dimension

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,  # do not pay attention to padding tokens
            "labels": torch.tensor(label, dtype=torch.long),
        }

"""## Train with early stopping"""

# with Early Stopping to prevent overfitting
def train_bert_early_stopping(model_name, cache_dir, device: torch.device, label_dict, train_sentences, train_labels, val_sentences, val_labels, base_path, epochs=12, early_stop_patience=6, batch_size=16):
    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_dict),
                                                               cache_dir=cache_dir, ignore_mismatched_sizes=True)
    model = model.to(device)
    results_csv_path = f"{base_path}/stats/{model_name.split('/')[-1]}_train.csv"
    f = open(results_csv_path, "w")
    f.write("epoch,train_loss,train_accuracy,val_loss,val_accuracy\n")

    train_dataset = SentenceDataset(
        sentences=train_sentences.to_list(),
        labels=train_labels.to_list(),
        tokenizer=tokenizer,
        label_dict=label_dict
    )
    val_dataset = SentenceDataset(
        sentences=val_sentences.to_list(),
        labels=val_labels.to_list(),
        tokenizer=tokenizer,
        label_dict=label_dict
    )

    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)

    optimizer = AdamW(model.parameters(), lr=2e-5)
    best_val_loss = float('inf')
    best_epoch = 0
    no_improvement_counter = 0

    # Learning rate scheduler setup
    total_steps = len(train_dataloader) * epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0

        description = f"Training Epoch {epoch + 1}"
        progress_bar = tqdm(train_dataloader, desc=description, colour='green')
        for batch in progress_bar:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)

            loss = outputs.loss
            train_loss += loss.item()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            _, predicted = torch.max(outputs.logits, dim=1)
            train_total += batch['labels'].size(0)
            train_correct += (predicted == batch['labels']).sum().item()
            progress_bar.set_postfix({"Loss": loss.item()})

        train_average_loss = train_loss / len(train_dataloader)
        train_accuracy = train_correct / train_total
        print(f"\nTrain Loss: {train_average_loss:.4f} | Train Accuracy: {train_accuracy:.4f}")

        # Evaluate the model
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            description = f"Validation Epoch {epoch + 1}"
            progress_bar = tqdm(val_dataloader, desc=description, colour='yellow')
            for batch in progress_bar:
                batch = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**batch)

                loss = outputs.loss
                val_loss += loss.item()

                _, predicted = torch.max(outputs.logits, dim=1)
                val_total += batch['labels'].size(0)
                val_correct += (predicted == batch['labels']).sum().item()

            val_average_loss = val_loss / len(val_dataloader)
            val_accuracy = val_correct / val_total
            print(f"Validation Loss: {val_average_loss:.4f} | Validation Accuracy: {val_accuracy:.4f}")
            print('*' * 50)
        f.write(
            f"{epoch + 1},{train_average_loss:.4f},{train_accuracy:.4f},{val_average_loss:.4f},{val_accuracy:.4f}\n")

        # Early stopping and saving the best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
            no_improvement_counter = 0
            print(f"Saving new best model at epoch {epoch + 1}")
            output_dir = f"{base_path}/models/{model_name}/best"
            model.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)
        else:
            no_improvement_counter += 1
            if no_improvement_counter >= early_stop_patience:
                print(f"Early stopping at epoch {epoch + 1}. Best epoch: {best_epoch + 1}")
                break

    f.close()

"""## Train with L2"""

# L2 regularization with AdamW (better training with better ثبات)
def train_bert_with_l2(model_name, cache_dir, device, label_dict, train_sentences, train_labels, val_sentences, val_labels, base_path, epochs=12, weight_decay=1e-3, batch_size=16):
    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_dict),
                                                               cache_dir=cache_dir, ignore_mismatched_sizes=True)
    # ignore_mismatched_sizes -> ignore the mismatched sizes between the model and the pretrained model
    # e.g. digikala: 2 class -> we have 7 class

    model.to(device)
    results_csv_path = os.path.join(base_path, "stats", f"{model_name.split('/')[-1]}_train.csv")

    with open(results_csv_path, "w") as f:
        f.write("epoch,train_loss,train_accuracy,val_loss,val_accuracy\n")

        train_dataset = SentenceDataset(sentences=train_sentences.to_list(), labels=train_labels.to_list(),
                                        tokenizer=tokenizer, label_dict=label_dict)
        val_dataset = SentenceDataset(sentences=val_sentences.to_list(), labels=val_labels.to_list(),
                                      tokenizer=tokenizer, label_dict=label_dict)

        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_dataloader = DataLoader(val_dataset, batch_size=batch_size)

        optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=weight_decay)  # with lambda parameter
        total_steps = len(train_dataloader) * epochs
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

        best_val_loss = float('inf')
        for epoch in range(epochs):
            model.train()
            train_loss, train_correct, train_total = 0, 0, 0
            progress_bar = tqdm(train_dataloader, desc=f"Training Epoch {epoch + 1}", colour='green')
            for batch in progress_bar:
                batch = {k: v.to(device) for k, v in batch.items()}
                outputs = model(**batch)

                loss = outputs.loss
                train_loss += loss.item()

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                scheduler.step()

                _, predicted = torch.max(outputs.logits, dim=1)
                train_total += batch['labels'].size(0)
                train_correct += (predicted == batch['labels']).sum().item()

            train_average_loss = train_loss / len(train_dataloader)
            train_accuracy = train_correct / train_total
            print(f"Epoch {epoch + 1}: Train Loss: {train_average_loss:.4f} | Train Accuracy: {train_accuracy:.4f}")

            model.eval()
            val_loss, val_correct, val_total = 0, 0, 0
            with torch.no_grad():
                for batch in tqdm(val_dataloader, desc=f"Validation Epoch {epoch + 1}", colour='yellow'):
                    batch = {k: v.to(device) for k, v in batch.items()}
                    outputs = model(**batch)

                    loss = outputs.loss
                    val_loss += loss.item()

                    _, predicted = torch.max(outputs.logits, dim=1)
                    val_total += batch['labels'].size(0)
                    val_correct += (predicted == batch['labels']).sum().item()

                val_average_loss = val_loss / len(val_dataloader)
                val_accuracy = val_correct / val_total
                print(f"Epoch {epoch + 1}: Val Loss: {val_average_loss:.4f} | Val Accuracy: {val_accuracy:.4f}")
                print('*' * 50)

            f.write(
                f"{epoch + 1},{train_average_loss:.4f},{train_accuracy:.4f},{val_average_loss:.4f},{val_accuracy:.4f}\n")

            if val_average_loss < best_val_loss:  # best result is the lowest validation loss
                best_val_loss = val_average_loss
                output_dir = os.path.join(base_path, "models", model_name, "best")
                if not os.path.exists(output_dir):
                    os.makedirs(output_dir)
                model.save_pretrained(output_dir)
                tokenizer.save_pretrained(output_dir)
                print(
                    f"Epoch {epoch + 1}: New best model saved with val_loss {best_val_loss:.4f} & val_acc {val_accuracy:.4f}")

"""## Test BERT"""

def test_bert(model_name, cache_dir, device: torch.device, label_dict, test_sentences, test_labels, base_path, batch_size=16, use_url=False):
    # Load the best model
    if use_url:
        model_path = model_name
    else:
        model_path = f"{base_path}/models/{model_name}/best"
    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(label_dict),
                                                               cache_dir=cache_dir, ignore_mismatched_sizes=True)
    model = model.to(device)
    results_csv_path = f"{base_path}/stats/{model_name.split('/')[-1]}_test.csv"
    f = open(results_csv_path, "w")
    f.write("test_loss,test_accuracy,precision,recall,f1\n")

    test_dataset = SentenceDataset(
        sentences=test_sentences,
        labels=test_labels,
        tokenizer=tokenizer,
        label_dict=label_dict
    )

    # Assuming `test_dataset` is an instance of `SentenceDataset` and already defined
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

    # Test the model
    model.eval()
    test_loss = 0
    test_correct = 0
    test_total = 0
    predictions, true_labels = [], []

    with torch.no_grad():
        progress_bar = tqdm(test_dataloader, desc="Testing", colour='blue')
        for batch in progress_bar:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)

            loss = outputs.loss
            test_loss += loss.item()

            _, predicted = torch.max(outputs.logits, dim=1)
            test_total += batch['labels'].size(0)
            test_correct += (predicted == batch['labels']).sum().item()

            # Collect the predictions and true labels for each batch
            predictions.extend(predicted.view(-1).cpu().numpy())
            true_labels.extend(batch['labels'].view(-1).cpu().numpy())

        # Calculate the average loss and accuracy over all test data
        test_average_loss = test_loss / len(test_dataloader)
        test_accuracy = test_correct / test_total
        print(f"Test Loss: {test_average_loss:.4f} | Test Accuracy: {test_accuracy:.4f}")

        # Compute precision, recall, and F1 score
        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')
        # average='macro': calculate metrics for each label, and find their unweighted mean (each class is equally weighted and has same value)

        print(f"Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}")
        print('*' * 50)
        f.write(f"{test_average_loss:.4f},{test_accuracy:.4f},{precision:.4f},{recall:.4f},{f1:.4f}\n")
        f.close()

"""## Predict BERT"""

def predict_bert(model_name, cache_dir, device, label_dict, text, base_path, use_url=False):
    # Load the model and tokenizer
    if use_url:
        model_path = model_name
    else:
        model_path = f"{base_path}/models/{model_name}/best"
    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_path, cache_dir=cache_dir)
    model = model.to(device)
    model.eval()  # Set the model to evaluation mode

    # Tokenize the input text
    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors="pt").to(device)

    # Predict
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # Convert logits to probabilities (optional)
    probabilities = torch.softmax(logits, dim=1)

    # Get the predicted label index
    predicted_label_index = logits.argmax(dim=1).item()

    # Map the predicted label index to its corresponding label name
    predicted_label_name = {v: k for k, v in label_dict.items()}[predicted_label_index]

    return predicted_label_name, probabilities[0][predicted_label_index].item()

"""## Evaluate BERT"""

# gets a csv, and cleans it, and evaluates it: returns accuracy, precision, recall, f1
def evaluate_bert(model_name, cache_dir, device, label_dict, csv_path, base_path, batch_size=16, use_url=False):
    df = pd.read_csv(csv_path)
    df['text'] = df['text'].apply(clean_text)
    df['label'] = df['label'].apply(lambda x: x.upper())
    test_sentences, test_labels = df['text'], df['label']

    test_dataset = SentenceDataset(
        sentences=test_sentences,
        labels=test_labels,
        tokenizer=AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir),
        label_dict=label_dict
    )
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

    # Load the best model
    if use_url:
        model_path = model_name
    else:
        model_path = f"{base_path}/models/{model_name}/best"
    tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=cache_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(label_dict), cache_dir=cache_dir, ignore_mismatched_sizes=True)
    model = model.to(device)

    # Test the model
    model.eval()
    test_loss = 0
    test_correct = 0
    test_total = 0
    predictions, true_labels = [], []

    with torch.no_grad():
        progress_bar = tqdm(test_dataloader, desc="Testing", colour='blue')
        for batch in progress_bar:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)

            loss = outputs.loss
            test_loss += loss.item()

            _, predicted = torch.max(outputs.logits, dim=1)
            test_total += batch['labels'].size(0)
            test_correct += (predicted == batch['labels']).sum().item()

            # Collect the predictions and true labels for each batch
            predictions.extend(predicted.view(-1).cpu().numpy())
            true_labels.extend(batch['labels'].view(-1).cpu().numpy())

        # Calculate the average loss and accuracy over all test data
        test_average_loss = test_loss / len(test_dataloader)
        test_accuracy = test_correct / test_total
        print(f"Test Loss: {test_average_loss:.4f} | Test Accuracy: {test_accuracy:.4f}")

        # Compute precision, recall, and F1 score
        precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')
        print(f"Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}")

"""# Load datasets and Preprocess"""

# pd.set_option('future.no_silent_downcasting', True)  # noqa

# Load datasets
train_df = pd.read_csv(f"{base_path}/train.tsv", sep="\t", header=None, names=["sentence", "label"])
test_df = pd.read_csv(f"{base_path}/test.tsv", sep="\t", header=None, names=["sentence", "label"])

# print distinct labels
print(train_df.label.unique())
print(test_df.label.unique())
print(set(train_df.label.unique()) == set(test_df.label.unique()))
print('*' * 50)

print(label_dict)
print(train_df.loc[0, "sentence"])
print(train_df.loc[0, "label"])
print('*' * 50)

print(train_df.dtypes)

"""## Raw: This is time consuming because of single thread"""

# clean data and save
tqdm.pandas()
if not os.path.exists(f"{base_path}/train_cleaned.tsv"):
    train_df['sentence'] = train_df['sentence'].progress_apply(clean_text)
    train_df.to_csv(f"{base_path}/train_cleaned.tsv", sep="\t", index=False)
if not os.path.exists(f"{base_path}/test_cleaned.tsv"):
    test_df['sentence'] = test_df['sentence'].progress_apply(clean_text)
    test_df.to_csv(f"{base_path}/test_cleaned.tsv", sep="\t", index=False)

# Function to apply preprocessing in parallel using joblib
# def parallel_apply(df, func):
#     processed_sentences = Parallel(n_jobs=-1)(
#         delayed(func)(text=sentence) for sentence in df['sentence'])
#     return processed_sentences
#
#
# if not os.path.exists(f"{base_path}/train_cleaned.tsv"):
#     train_df['sentence'] = parallel_apply(train_df, combined_preprocess)
#
# if not os.path.exists(f"{base_path}/test_cleaned.tsv"):
#     test_df['sentence'] = parallel_apply(test_df, combined_preprocess)

"""## Clean: We prefer to load saved clean ones"""

# First row is header
train_df = pd.read_csv(f"{base_path}/train_cleaned.tsv", sep="\t")
test_df = pd.read_csv(f"{base_path}/test_cleaned.tsv", sep="\t")

print(train_df.loc[0:5, "sentence"])
print(test_df.loc[0:5, "sentence"])
print(test_df.loc[6, "sentence"])

"""## Number of samples per label"""

# Split train dataset for validation
train_sentences, val_sentences, train_labels, val_labels = train_test_split(
    train_df['sentence'], train_df['label'], test_size=0.1, random_state=42
)
print(f"Number of training sentences: {len(train_sentences)}")
print(f"Number of validation sentences: {len(val_sentences)}")
print(f"Number of test sentences: {len(test_df)}")

"""# Varations of BERT and XLM Roberta"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

model_names = [
    # 'HooshvareLab/bert-fa-base-uncased-sentiment-snappfood',  # noqa
    # 'HooshvareLab/bert-fa-base-uncased-sentiment-digikala',  # noqa
    # 'HooshvareLab/bert-fa-base-uncased',  # noqa
    # 'HooshvareLab/bert-fa-zwnj-base',  # ParsBERT (v3.0) # noqa

    # 'FacebookAI/xlm-roberta-base',  # noqa
    # 'FacebookAI/xlm-roberta-large',  # noqa

    # 'HooshvareLab/roberta-fa-zwnj-base',  # noqa
    'pedramyazdipoor/persian_xlm_roberta_large',  # keeping the best one for showcasing
]
cache_dir = f'{base_path}/models/huggingface_cache'

"""## Train Models"""

for model_name in model_names:
    print(f"Model name: {model_name}")
    train_bert_with_l2(
        model_name=model_name,
        cache_dir=cache_dir,
        device=device,
        label_dict=label_dict,
        train_sentences=train_sentences,
        train_labels=train_labels,
        val_sentences=val_sentences,
        val_labels=val_labels,
        base_path=base_path,
        batch_size=20,
        epochs=8,
    )

"""## Free GPU memory"""

# !sudo fuser -v /dev/nvidia*
import gc
torch.cuda.empty_cache()
gc.collect()

"""## Test fine-tuned models"""

for model_name in model_names:
    print(f"Model name: {model_name}")
    test_bert(
        model_name=model_name,
        cache_dir=cache_dir,
        device=device,
        label_dict=label_dict,
        test_sentences=test_df['sentence'].to_list(),
        test_labels=test_df['label'].to_list(),
        base_path=base_path,
        batch_size=16,
    )

"""## Test with the best model"""

model_name = "farzanrahmani/persian_xlm_roberta_large"
print(f"Model name: {model_name}")
test_bert(
    model_name=model_name,
    cache_dir=cache_dir,
    device=device,
    label_dict=label_dict,
    test_sentences=test_df['sentence'].to_list(),
    test_labels=test_df['label'].to_list(),
    base_path=base_path,
    batch_size=16,
    use_url=True
)

"""# Push the models to the huggingface"""

notebook_login()
TOKEN_OF_HAMED='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'
TOKEN_OF_FARZAN='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'

TOKEN=TOKEN_OF_HAMED
path_and_name = [
    # (f"{base_path}/models/HooshvareLab/bert-fa-base-uncased-sentiment-snappfood/best", "bert-fa-base-uncased-sentiment-snappfood"),
    # (f"{base_path}/models/FacebookAI/xlm-roberta-base/best", "xlm-roberta-base"),
    # (f"{base_path}/models/FacebookAI/xlm-roberta-large/best", "xlm-roberta-large"),
    (f"{base_path}/models/pedramyazdipoor/persian_xlm_roberta_large/best", "persian_xlm_roberta_large"),
    # (f"{base_path}/models/HooshvareLab/roberta-fa-zwnj-base/best", "roberta-fa-zwnj-base")
]

for item in path_and_name:
  tokenizer = AutoTokenizer.from_pretrained(item[0], cache_dir=cache_dir)
  model = AutoModelForSequenceClassification.from_pretrained(item[0], num_labels=len(label_dict), cache_dir=cache_dir, ignore_mismatched_sizes=True)
  model.push_to_hub(item[1], use_auth_token=TOKEN)
  tokenizer.push_to_hub(item[1],  use_auth_token=TOKEN, commit_message="Upload Tokenizer")

"""# Example usage"""

# model_name = "hamedhf/persian_xlm_roberta_large"
model_name = "farzanrahmani/persian_xlm_roberta_large"
texts = [
    "من این محصول رو دوست داشتم",  # noqa
    "حالم از این وضع بهم می‌خوره",  # noqa
]

for text in texts:
    label, probability = predict_bert(model_name, cache_dir, device, label_dict, text, base_path, use_url=True)
    print(f"Text: {text}")
    print(f"Predicted label: {label} with probability: {probability}")
    print('*' * 50)

csv_content = f"""text,label
من این محصول رو دوست داشتم,happy
حالم از این وضع بهم می‌خوره,sad
"""
csv_path = f"{base_path}/test_custom.csv"
with open(csv_path, "w") as f:
    f.write(csv_content)

model_name = "hamedhf/persian_xlm_roberta_large"
evaluate_bert(model_name, cache_dir, device, label_dict, csv_path, base_path, use_url=True)

