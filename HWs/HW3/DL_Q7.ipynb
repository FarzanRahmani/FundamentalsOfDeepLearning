{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 (First method)"
      ],
      "metadata": {
        "id": "bHINloRyhA9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Inception module is a type of convolutional neural network architecture that was first introduced in the GoogleNet paper. It is designed to improve the efficiency of feature extraction by using parallel branches of convolutions with different filter sizes. This allows the network to extract a wider range of features from the input image."
      ],
      "metadata": {
        "id": "6i7hugm2iaGA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvcZf28jXfz5",
        "outputId": "820917d8-95b7-4abb-e234-a0fb95a7ff0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n",
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 48s 22ms/step - loss: 1.3319 - accuracy: 0.5249 - val_loss: 1.1410 - val_accuracy: 0.5905\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.9839 - accuracy: 0.6531 - val_loss: 0.9110 - val_accuracy: 0.6854\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 38s 24ms/step - loss: 0.8472 - accuracy: 0.7037 - val_loss: 0.8792 - val_accuracy: 0.6959\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 42s 27ms/step - loss: 0.7582 - accuracy: 0.7340 - val_loss: 0.8489 - val_accuracy: 0.7149\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6844 - accuracy: 0.7602 - val_loss: 0.7623 - val_accuracy: 0.7403\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6222 - accuracy: 0.7807 - val_loss: 0.7747 - val_accuracy: 0.7432\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5750 - accuracy: 0.7986 - val_loss: 0.7384 - val_accuracy: 0.7539\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.5194 - accuracy: 0.8192 - val_loss: 0.7837 - val_accuracy: 0.7552\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 40s 25ms/step - loss: 0.4864 - accuracy: 0.8294 - val_loss: 0.7087 - val_accuracy: 0.7710\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 39s 25ms/step - loss: 0.4478 - accuracy: 0.8421 - val_loss: 0.7357 - val_accuracy: 0.7633\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f6a93fecfa0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define the Inception module\n",
        "def inception_module(x):\n",
        "    branch1 = Conv2D(filters=32, kernel_size=(1, 1), activation='relu', strides=(1, 1), padding='same')(x)\n",
        "\n",
        "    branch2 = Conv2D(filters=32, kernel_size=(1, 1), activation='relu', strides=(1, 1), padding='same')(x)\n",
        "    branch2 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=(1, 1), padding='same')(branch2)\n",
        "\n",
        "    branch3 = Conv2D(filters=64, kernel_size=(1, 1), activation='relu', strides=(1, 1), padding='same')(x)\n",
        "    branch3 = Conv2D(filters=128, kernel_size=(3, 3), activation='relu', strides=(1, 1), padding='same')(branch3)\n",
        "\n",
        "    branch4 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "    branch4 = Conv2D(filters=32, kernel_size=(1, 1), activation='relu', strides=(1, 1), padding='same')(branch4)\n",
        "\n",
        "    concatenated = tf.concat([branch1, branch2, branch3, branch4], axis=-1)\n",
        "    return concatenated\n",
        "\n",
        "# Define the convolutional neural network model\n",
        "inputs = Input(shape=(32, 32, 3))\n",
        "\n",
        "x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=(1, 1), padding='same')(inputs)\n",
        "x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "x = inception_module(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=512, activation='relu')(x)\n",
        "outputs = Dense(units=10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True, zoom_range=0.2)\n",
        "\n",
        "# Train the model\n",
        "model.fit(datagen.flow(x_train, y_train, batch_size=32), epochs=10, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inception module: The Inception module is a convolutional neural network architecture that combines filters of different sizes in a single layer. This allows the network to capture a wider range of spatial and frequency information from the input data.\n",
        "\n",
        "Stride parameter: The stride parameter in a convolutional layer determines how many pixels the filter moves across the input feature map after each convolution operation. A larger stride parameter will reduce the spatial dimensions of the output feature map.\n",
        "\n",
        "Convolutional layers: Convolutional layers are the basic building blocks of convolutional neural networks. They apply filters to the input data to extract features. The key features of convolutional layers are their ability to extract local patterns and their ability to share weights across the input data.\n",
        "\n",
        "Regularization and data augmentation: Regularization techniques, such as dropout, can help to prevent overfitting. Data augmentation techniques, such as horizontal and vertical flipping, can be used to increase the size of the training dataset."
      ],
      "metadata": {
        "id": "BzA-KjtYiAEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 (Second method)"
      ],
      "metadata": {
        "id": "ivbu5KrfhMfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Libraries and Load CIFAR-10 Data:\n"
      ],
      "metadata": {
        "id": "dsGoilrXYHut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Flatten, Dense\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n"
      ],
      "metadata": {
        "id": "owbfiRXQYGM9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocess Data:\n"
      ],
      "metadata": {
        "id": "GkfgS_NEYKrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values to range [0, 1]\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n"
      ],
      "metadata": {
        "id": "gE_IEgZGYNI8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build the Inception Module:\n",
        "The Inception module comprises parallel convolutional branches of different kernel sizes, followed by concatenation."
      ],
      "metadata": {
        "id": "S19oG7lSYSYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_module(x):\n",
        "    branch_1x1 = Conv2D(16, (1, 1), padding='same', activation='relu')(x)\n",
        "\n",
        "    branch_3x3 = Conv2D(16, (1, 1), padding='same', activation='relu')(x)\n",
        "    branch_3x3 = Conv2D(32, (3, 3), padding='same', activation='relu')(branch_3x3)\n",
        "\n",
        "    branch_5x5 = Conv2D(16, (1, 1), padding='same', activation='relu')(x)\n",
        "    branch_5x5 = Conv2D(32, (5, 5), padding='same', activation='relu')(branch_5x5)\n",
        "\n",
        "    branch_pool = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "    branch_pool = Conv2D(16, (1, 1), padding='same', activation='relu')(branch_pool)\n",
        "\n",
        "    output = concatenate([branch_1x1, branch_3x3, branch_5x5, branch_pool], axis=-1)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "nLAydYv3YT-U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Build the CNN with Inception Module:\n"
      ],
      "metadata": {
        "id": "N9FN275LYYt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(32, 32, 3))\n",
        "\n",
        "conv1 = Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu')(input_layer)\n",
        "conv2 = Conv2D(64, (3, 3), strides=(2, 2), padding='same', activation='relu')(conv1)\n",
        "\n",
        "inception = inception_module(conv2)\n",
        "\n",
        "flatten = Flatten()(inception)\n",
        "dense = Dense(128, activation='relu')(flatten)\n",
        "output = Dense(10, activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output)\n"
      ],
      "metadata": {
        "id": "8psRVKUYYahU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Compile and Train the Model:\n"
      ],
      "metadata": {
        "id": "eswlaqXLYgbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_images, train_labels, epochs=20, batch_size=128, validation_data=(test_images, test_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV1NJJVPYiYU",
        "outputId": "04ca1768-28b6-49b2-a9f2-2896e0616adf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "391/391 [==============================] - 9s 16ms/step - loss: 1.4414 - accuracy: 0.4858 - val_loss: 1.1379 - val_accuracy: 0.6011\n",
            "Epoch 2/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.9961 - accuracy: 0.6539 - val_loss: 0.9997 - val_accuracy: 0.6511\n",
            "Epoch 3/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.8108 - accuracy: 0.7189 - val_loss: 0.8849 - val_accuracy: 0.6970\n",
            "Epoch 4/20\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.6700 - accuracy: 0.7686 - val_loss: 0.8897 - val_accuracy: 0.6967\n",
            "Epoch 5/20\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.5236 - accuracy: 0.8187 - val_loss: 0.9291 - val_accuracy: 0.7012\n",
            "Epoch 6/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.3858 - accuracy: 0.8660 - val_loss: 1.0452 - val_accuracy: 0.6891\n",
            "Epoch 7/20\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.2640 - accuracy: 0.9094 - val_loss: 1.1961 - val_accuracy: 0.6902\n",
            "Epoch 8/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.1579 - accuracy: 0.9468 - val_loss: 1.3856 - val_accuracy: 0.6862\n",
            "Epoch 9/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.1056 - accuracy: 0.9648 - val_loss: 1.6491 - val_accuracy: 0.6849\n",
            "Epoch 10/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.0860 - accuracy: 0.9711 - val_loss: 1.7847 - val_accuracy: 0.6861\n",
            "Epoch 11/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.0719 - accuracy: 0.9759 - val_loss: 1.8813 - val_accuracy: 0.6736\n",
            "Epoch 12/20\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.0672 - accuracy: 0.9769 - val_loss: 1.9919 - val_accuracy: 0.6736\n",
            "Epoch 13/20\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.0566 - accuracy: 0.9810 - val_loss: 2.1602 - val_accuracy: 0.6732\n",
            "Epoch 14/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.0543 - accuracy: 0.9816 - val_loss: 2.2011 - val_accuracy: 0.6834\n",
            "Epoch 15/20\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.0503 - accuracy: 0.9828 - val_loss: 2.3533 - val_accuracy: 0.6727\n",
            "Epoch 16/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.0559 - accuracy: 0.9812 - val_loss: 2.3019 - val_accuracy: 0.6752\n",
            "Epoch 17/20\n",
            "391/391 [==============================] - 5s 14ms/step - loss: 0.0471 - accuracy: 0.9839 - val_loss: 2.5658 - val_accuracy: 0.6738\n",
            "Epoch 18/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.0533 - accuracy: 0.9818 - val_loss: 2.4602 - val_accuracy: 0.6789\n",
            "Epoch 19/20\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.0418 - accuracy: 0.9859 - val_loss: 2.4561 - val_accuracy: 0.6667\n",
            "Epoch 20/20\n",
            "391/391 [==============================] - 5s 13ms/step - loss: 0.0378 - accuracy: 0.9871 - val_loss: 2.5791 - val_accuracy: 0.6695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model"
      ],
      "metadata": {
        "id": "lCWgqLzSYrtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk6-kY4iYnRm",
        "outputId": "8f7f0e2a-7dcd-45af-c152-5ce5ca97aca6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 2.5790 - accuracy: 0.6694\n",
            "Test accuracy: 0.6693999767303467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Points:\n",
        "### Inception Module:\n",
        "#### Purpose:\n",
        " The Inception module facilitates capturing features at multiple scales by using parallel convolutions of different kernel sizes (1x1, 3x3, 5x5) and pooling, allowing the network to learn diverse features and increase representational power.\n",
        "### Stride Parameter in Convolutional Layers:\n",
        "####Effect on Spatial Dimensions:\n",
        " A stride parameter > 1 reduces the spatial dimensions of feature maps. A stride of 2 reduces the spatial dimensions by a factor of 2, affecting the output size of the layer.\n",
        "### Importance of Convolutional Layers:\n",
        "#### Feature Extraction:\n",
        " Convolutional layers perform feature extraction by applying learnable filters to input data. Different filters capture different features (edges, textures) and play a crucial role in learning hierarchical representations."
      ],
      "metadata": {
        "id": "7mRhYrGPZTd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3 (Third method)\n",
        " We can use various techniques like data augmentation, dropout, pooling, and more to improve the model's accuracy."
      ],
      "metadata": {
        "id": "Rj5zXRsJh0TJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First, let's import the necessary libraries and load the CIFAR-10 dataset:\n",
        "\n"
      ],
      "metadata": {
        "id": "M-Ku31Duipm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "KQPUrzX_iq3r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and preprocess the CIFAR-10 dataset:\n",
        "\n"
      ],
      "metadata": {
        "id": "CyhKzguIit7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoded vectors\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "iukxrUmRivDK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next, let's define the Inception module:\n",
        "\n"
      ],
      "metadata": {
        "id": "WL0xpBPFi7Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inception_module(prev_layer, filters):\n",
        "    conv1x1 = Conv2D(filters=filters[0], kernel_size=(1, 1), activation='relu', padding='same')(prev_layer)\n",
        "\n",
        "    conv3x3 = Conv2D(filters=filters[1], kernel_size=(3, 3), activation='relu', padding='same')(prev_layer)\n",
        "\n",
        "    conv5x5 = Conv2D(filters=filters[2], kernel_size=(5, 5), activation='relu', padding='same')(prev_layer)\n",
        "\n",
        "    maxpool = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(prev_layer)\n",
        "    pool_conv = Conv2D(filters=filters[3], kernel_size=(1, 1), activation='relu', padding='same')(maxpool)\n",
        "\n",
        "    # Concatenate the outputs of different convolutions\n",
        "    output = concatenate([conv1x1, conv3x3, conv5x5, pool_conv], axis=-1)\n",
        "    return output"
      ],
      "metadata": {
        "id": "PwLdI543i8L7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, let's build the CNN model with Inception module and other layers:\n"
      ],
      "metadata": {
        "id": "qTPbwV1bjB3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(32, 32, 3))\n",
        "\n",
        "# First Convolutional layer\n",
        "conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "# Apply Inception module\n",
        "inception1 = inception_module(conv1, [32, 32, 32, 32])\n",
        "\n",
        "# Additional Convolutional layers\n",
        "conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(inception1)\n",
        "conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "# Flatten the output for Dense layers\n",
        "flatten = Flatten()(conv2)\n",
        "\n",
        "# Dense layers\n",
        "dense1 = Dense(256, activation='relu')(flatten)\n",
        "dense1 = Dropout(0.5)(dense1)\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense1)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "JjqccCrgjDQq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile the model and define the training parameters:\n",
        "\n"
      ],
      "metadata": {
        "id": "wuAmP-F1jLc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkXpGscVjM0j",
        "outputId": "59be7e69-38a0-46b8-9f6c-52988a5c008d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "781/781 [==============================] - 40s 46ms/step - loss: 1.6481 - accuracy: 0.4003 - val_loss: 1.3300 - val_accuracy: 0.5313\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.3183 - accuracy: 0.5278 - val_loss: 1.0763 - val_accuracy: 0.6199\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.1946 - accuracy: 0.5782 - val_loss: 0.9964 - val_accuracy: 0.6439\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.1061 - accuracy: 0.6102 - val_loss: 0.8739 - val_accuracy: 0.6939\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 1.0430 - accuracy: 0.6336 - val_loss: 0.9179 - val_accuracy: 0.6751\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 0.9932 - accuracy: 0.6515 - val_loss: 0.9343 - val_accuracy: 0.6847\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.9622 - accuracy: 0.6649 - val_loss: 0.8114 - val_accuracy: 0.7168\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 0.9200 - accuracy: 0.6800 - val_loss: 0.7887 - val_accuracy: 0.7245\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.9027 - accuracy: 0.6835 - val_loss: 0.7650 - val_accuracy: 0.7350\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.8799 - accuracy: 0.6931 - val_loss: 0.7406 - val_accuracy: 0.7427\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 0.8593 - accuracy: 0.7023 - val_loss: 0.7855 - val_accuracy: 0.7313\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.8424 - accuracy: 0.7098 - val_loss: 0.7397 - val_accuracy: 0.7461\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 32s 40ms/step - loss: 0.8284 - accuracy: 0.7127 - val_loss: 0.7845 - val_accuracy: 0.7358\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 0.8079 - accuracy: 0.7203 - val_loss: 0.7119 - val_accuracy: 0.7548\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.7942 - accuracy: 0.7240 - val_loss: 0.7477 - val_accuracy: 0.7444\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 37s 47ms/step - loss: 0.7853 - accuracy: 0.7269 - val_loss: 0.6834 - val_accuracy: 0.7629\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 32s 40ms/step - loss: 0.7764 - accuracy: 0.7324 - val_loss: 0.6483 - val_accuracy: 0.7770\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 0.7597 - accuracy: 0.7374 - val_loss: 0.6975 - val_accuracy: 0.7592\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 32s 40ms/step - loss: 0.7486 - accuracy: 0.7407 - val_loss: 0.7271 - val_accuracy: 0.7528\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.7465 - accuracy: 0.7424 - val_loss: 0.6551 - val_accuracy: 0.7780\n",
            "Epoch 21/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.7338 - accuracy: 0.7472 - val_loss: 0.6297 - val_accuracy: 0.7838\n",
            "Epoch 22/30\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.7277 - accuracy: 0.7480 - val_loss: 0.6575 - val_accuracy: 0.7698\n",
            "Epoch 23/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.7101 - accuracy: 0.7545 - val_loss: 0.6437 - val_accuracy: 0.7812\n",
            "Epoch 24/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.7048 - accuracy: 0.7544 - val_loss: 0.6910 - val_accuracy: 0.7718\n",
            "Epoch 25/30\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 0.6991 - accuracy: 0.7584 - val_loss: 0.6685 - val_accuracy: 0.7739\n",
            "Epoch 26/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.6950 - accuracy: 0.7596 - val_loss: 0.6401 - val_accuracy: 0.7853\n",
            "Epoch 27/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.6859 - accuracy: 0.7646 - val_loss: 0.6233 - val_accuracy: 0.7908\n",
            "Epoch 28/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 0.6862 - accuracy: 0.7614 - val_loss: 0.6584 - val_accuracy: 0.7763\n",
            "Epoch 29/30\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 0.6747 - accuracy: 0.7653 - val_loss: 0.6617 - val_accuracy: 0.7794\n",
            "Epoch 30/30\n",
            "781/781 [==============================] - 32s 40ms/step - loss: 0.6658 - accuracy: 0.7706 - val_loss: 0.6225 - val_accuracy: 0.7939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model:\n",
        "\n"
      ],
      "metadata": {
        "id": "Evs6TsxbjRUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test set\n",
        "_, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkjPHukjjSra",
        "outputId": "dcf435d5-2498-4be8-b775-49e93f301e6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.7939\n",
            "Test accuracy: 79.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4 (Forth method)\n",
        "### add L2 regularization to the Convolutional layers and Dense layers to see the results"
      ],
      "metadata": {
        "id": "Qzh0BQgKkDFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Define the regularization parameter\n",
        "regularization_rate = 0.001  # You can adjust this value\n",
        "\n",
        "input_layer = Input(shape=(32, 32, 3))\n",
        "\n",
        "# First Convolutional layer with L2 regularization\n",
        "conv1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(regularization_rate))(input_layer)\n",
        "conv1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "# Apply Inception module\n",
        "inception1 = inception_module(conv1, [32, 32, 32, 32])\n",
        "\n",
        "# Additional Convolutional layers with L2 regularization\n",
        "conv2 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(regularization_rate))(inception1)\n",
        "conv2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "# Flatten the output for Dense layers\n",
        "flatten = Flatten()(conv2)\n",
        "\n",
        "# Dense layers with L2 regularization\n",
        "dense1 = Dense(256, activation='relu', kernel_regularizer=l2(regularization_rate))(flatten)\n",
        "dense1 = Dropout(0.5)(dense1)\n",
        "\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense1)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "zMSUiVYzkJdF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile the model and define the training parameters:\n",
        "\n"
      ],
      "metadata": {
        "id": "B1Pa6APDkRfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjCrRWI2kSkb",
        "outputId": "011452fb-26d9-4312-af77-5eb536cf2006"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "781/781 [==============================] - 36s 43ms/step - loss: 1.7983 - accuracy: 0.3807 - val_loss: 1.5050 - val_accuracy: 0.4913\n",
            "Epoch 2/30\n",
            "781/781 [==============================] - 32s 40ms/step - loss: 1.5302 - accuracy: 0.4951 - val_loss: 1.3507 - val_accuracy: 0.5543\n",
            "Epoch 3/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 1.4479 - accuracy: 0.5315 - val_loss: 1.2351 - val_accuracy: 0.6027\n",
            "Epoch 4/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 1.3887 - accuracy: 0.5552 - val_loss: 1.1894 - val_accuracy: 0.6319\n",
            "Epoch 5/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 1.3475 - accuracy: 0.5751 - val_loss: 1.2259 - val_accuracy: 0.6231\n",
            "Epoch 6/30\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 1.3139 - accuracy: 0.5930 - val_loss: 1.1979 - val_accuracy: 0.6355\n",
            "Epoch 7/30\n",
            "781/781 [==============================] - 31s 40ms/step - loss: 1.2879 - accuracy: 0.6030 - val_loss: 1.1051 - val_accuracy: 0.6708\n",
            "Epoch 8/30\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 1.2648 - accuracy: 0.6148 - val_loss: 1.0927 - val_accuracy: 0.6793\n",
            "Epoch 9/30\n",
            "781/781 [==============================] - 35s 44ms/step - loss: 1.2525 - accuracy: 0.6206 - val_loss: 1.0905 - val_accuracy: 0.6806\n",
            "Epoch 10/30\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.2292 - accuracy: 0.6291 - val_loss: 1.1165 - val_accuracy: 0.6667\n",
            "Epoch 11/30\n",
            "781/781 [==============================] - 33s 43ms/step - loss: 1.2131 - accuracy: 0.6344 - val_loss: 1.1687 - val_accuracy: 0.6627\n",
            "Epoch 12/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.1999 - accuracy: 0.6444 - val_loss: 1.0059 - val_accuracy: 0.7106\n",
            "Epoch 13/30\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 1.1798 - accuracy: 0.6511 - val_loss: 1.0933 - val_accuracy: 0.6852\n",
            "Epoch 14/30\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 1.1810 - accuracy: 0.6494 - val_loss: 1.0288 - val_accuracy: 0.7083\n",
            "Epoch 15/30\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 1.1673 - accuracy: 0.6558 - val_loss: 1.0116 - val_accuracy: 0.7107\n",
            "Epoch 16/30\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 1.1560 - accuracy: 0.6620 - val_loss: 1.0698 - val_accuracy: 0.6947\n",
            "Epoch 17/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.1429 - accuracy: 0.6645 - val_loss: 0.9985 - val_accuracy: 0.7185\n",
            "Epoch 18/30\n",
            "781/781 [==============================] - 32s 41ms/step - loss: 1.1402 - accuracy: 0.6707 - val_loss: 0.9889 - val_accuracy: 0.7198\n",
            "Epoch 19/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.1342 - accuracy: 0.6710 - val_loss: 0.9910 - val_accuracy: 0.7178\n",
            "Epoch 20/30\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 1.1282 - accuracy: 0.6732 - val_loss: 0.9715 - val_accuracy: 0.7313\n",
            "Epoch 21/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.1098 - accuracy: 0.6817 - val_loss: 0.9378 - val_accuracy: 0.7430\n",
            "Epoch 22/30\n",
            "781/781 [==============================] - 36s 46ms/step - loss: 1.1131 - accuracy: 0.6791 - val_loss: 1.0264 - val_accuracy: 0.7201\n",
            "Epoch 23/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.1095 - accuracy: 0.6846 - val_loss: 1.0686 - val_accuracy: 0.7032\n",
            "Epoch 24/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.1070 - accuracy: 0.6845 - val_loss: 0.9518 - val_accuracy: 0.7361\n",
            "Epoch 25/30\n",
            "781/781 [==============================] - 34s 43ms/step - loss: 1.1035 - accuracy: 0.6830 - val_loss: 0.9649 - val_accuracy: 0.7347\n",
            "Epoch 26/30\n",
            "781/781 [==============================] - 35s 45ms/step - loss: 1.0957 - accuracy: 0.6879 - val_loss: 0.9261 - val_accuracy: 0.7451\n",
            "Epoch 27/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.0822 - accuracy: 0.6925 - val_loss: 0.9843 - val_accuracy: 0.7204\n",
            "Epoch 28/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.0821 - accuracy: 0.6952 - val_loss: 0.9782 - val_accuracy: 0.7323\n",
            "Epoch 29/30\n",
            "781/781 [==============================] - 34s 44ms/step - loss: 1.0803 - accuracy: 0.6909 - val_loss: 0.9958 - val_accuracy: 0.7257\n",
            "Epoch 30/30\n",
            "781/781 [==============================] - 33s 42ms/step - loss: 1.0773 - accuracy: 0.6961 - val_loss: 0.9490 - val_accuracy: 0.7409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model:\n",
        "\n"
      ],
      "metadata": {
        "id": "GMRZjVj5kWAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test set\n",
        "_, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnfLknwmkXTT",
        "outputId": "02337760-143a-45f1-f702-2e84e2504351"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 0.9490 - accuracy: 0.7409\n",
            "Test accuracy: 74.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This code creates a CNN model with an Inception module and utilizes data augmentation, dropout, and other techniques to achieve classification on the CIFAR-10 dataset."
      ],
      "metadata": {
        "id": "GvuUj_hzjczk"
      }
    }
  ]
}